{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c3neW1n6lrrW"
   },
   "source": [
    "# Data mining \n",
    "## Final Project\n",
    "## Μπριάκος Σπυρίδων 1115201700101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hqS3lDtmM3Bv"
   },
   "source": [
    "### Import python libraries, obtain .csv files and save 'cleaned' train,test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g3niIONHlrrZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import string\n",
    "import re\n",
    "from wordcloud import STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CsFtb_Lmlrrh"
   },
   "source": [
    "#### Obtain .csv files which we are going to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1315,
     "status": "ok",
     "timestamp": 1591785552634,
     "user": {
      "displayName": "Σπύρος Μπριάκος",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1ZSGs85hEnrevgOWpVj9cYnDNg1wvvPDbnumHGw=s64",
      "userId": "14497433372697123826"
     },
     "user_tz": -180
    },
    "id": "57nGQmwllrri",
    "outputId": "a52adf69-d448-47db-f056-95ce4a2402af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "train_df = pd.read_csv(r'/content/drive/My Drive/data/train.csv')\n",
    "test_df = pd.read_csv(r'/content/drive/My Drive/data/impermium_verification_labels.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vzYyBIzAlrrm"
   },
   "source": [
    "#### Convertion to lower cases and removal of links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A36jiJ23lrrn"
   },
   "outputs": [],
   "source": [
    "#Lower cases and removal of punctuations...\n",
    "for i, row in train_df.iterrows():\n",
    "    train_df.loc[i,'Comment']  =  train_df.loc[i,'Comment'].lower()\n",
    "    train_df.loc[i,'Comment'] = ''.join(ch for ch in train_df['Comment'][i] if ch not in set(string.punctuation))\n",
    "for i, row in test_df.iterrows():\n",
    "    test_df.loc[i,'Comment']  =  test_df.loc[i,'Comment'].lower()\n",
    "    test_df.loc[i,'Comment'] = ''.join(ch for ch in test_df['Comment'][i] if ch not in set(string.punctuation))\n",
    "\n",
    "#Declaration of function remove_tags, which will help us remove html tags.\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "    \n",
    "#Removal of html tags...\n",
    "train_df['Comment'] = train_df['Comment'].apply(lambda x: remove_tags(x))\n",
    "test_df['Comment'] = test_df['Comment'].apply(lambda x: remove_tags(x))\n",
    "    \n",
    "#Removal of links...\n",
    "train_df['Comment'] = train_df['Comment'].replace(r'http\\S+', '', regex=True).replace(r'www.\\S+', '', regex=True).replace(r'\"', '', regex=True)\n",
    "test_df['Comment'] = test_df['Comment'].replace(r'http\\S+', '', regex=True).replace(r'www.\\S+', '', regex=True).replace(r'\"', '', regex=True)\n",
    "\n",
    "#Removal of symbols...\n",
    "train_df['Comment'] = train_df['Comment'].replace(r'\\\\\\S+', '', regex=True).replace(r'/', '', regex=True)\n",
    "test_df['Comment'] = test_df['Comment'].replace(r'\\\\\\S+', '', regex=True).replace(r'/', '', regex=True)\n",
    "\n",
    "# train_df['Comment'].dropna(inplace=True)\n",
    "# test_df['Comment'].dropna(inplace=True)\n",
    "# train_df['Comment'] = train_df['Comment'].astype('U')\n",
    "# test_df['Comment'] = test_df['Comment'].astype('U')\n",
    "#Store cleaned train and test dataframes.\n",
    "train_df.to_csv(r'/content/drive/My Drive/cleaned_train.csv', index=False)\n",
    "test_df.to_csv(r'/content/drive/My Drive/cleaned_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ad-5irRFlrrt"
   },
   "source": [
    "### Naive Bayes Scores (first try)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GqwK1Mtklrru"
   },
   "source": [
    "#### Vectorization with BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9q3fn0Xzlrru",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Using simple CountVectorizer.\n",
    "count_vectorizer = CountVectorizer()\n",
    "bowX_train = count_vectorizer.fit_transform(train_df['Comment'].tolist()) \n",
    "bowX_test = count_vectorizer.transform(test_df['Comment'].tolist()) \n",
    "\n",
    "#Forming labels, which are going to help us with prediction and evaluation.\n",
    "label_train = train_df['Insult'].tolist()\n",
    "label_test = test_df['Insult'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N2ho58dFlrrz"
   },
   "source": [
    "#### First Scores of Naive Bayes, only with Bag_of_Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11556,
     "status": "ok",
     "timestamp": 1591785562899,
     "user": {
      "displayName": "Σπύρος Μπριάκος",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1ZSGs85hEnrevgOWpVj9cYnDNg1wvvPDbnumHGw=s64",
      "userId": "14497433372697123826"
     },
     "user_tz": -180
    },
    "id": "aTlN1ff3lrr0",
    "outputId": "8a417aab-9577-401f-b789-abec0be7a945"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of Bag-of-words with Naive-Bayes Classifier:  0.53\n",
      "Accuracy score of Bag-of-words with Naive-Bayes Classifier:  0.53\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "\n",
    "#Using Gaussian Naive Bayes Classifier and train data, then predict and evaluate score on test data.\n",
    "gnb1 = GaussianNB()\n",
    "gnb1.fit(bowX_train.toarray(), label_train)\n",
    "predictions = gnb1.predict(bowX_test.toarray())\n",
    "\n",
    "#F1 \n",
    "first_nb_f1_score = \"{:.2f}\".format(f1_score(label_test,predictions,average='weighted'))\n",
    "print('F1 score of Bag-of-words with Naive-Bayes Classifier: ' , first_nb_f1_score)\n",
    "\n",
    "#Accuracy\n",
    "first_nb_acc_score = \"{:.2f}\".format(accuracy_score(label_test,predictions))\n",
    "print('Accuracy score of Bag-of-words with Naive-Bayes Classifier: ' , first_nb_acc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1kCG11vylrr6"
   },
   "source": [
    "### Optimization of Naive Bayes with lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bexpL-tIlrr8"
   },
   "source": [
    "#### Obtain cleaned train,test dataframes, lemmatization and then Vectorization with BoW\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12433,
     "status": "ok",
     "timestamp": 1591785563788,
     "user": {
      "displayName": "Σπύρος Μπριάκος",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1ZSGs85hEnrevgOWpVj9cYnDNg1wvvPDbnumHGw=s64",
      "userId": "14497433372697123826"
     },
     "user_tz": -180
    },
    "id": "OeM5soviLYdE",
    "outputId": "70ae41e4-948d-427f-b60a-06e15b6372bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "train_df = pd.read_csv(r'/content/drive/My Drive/cleaned_train.csv')\n",
    "test_df = pd.read_csv(r'/content/drive/My Drive/cleaned_test.csv') \n",
    "\n",
    "# Lemmatization on data\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "#Functions, first lemmatize a word and second just concatenate strings of a list to a simple string.\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w, pos=\"v\") for w in w_tokenizer.tokenize(text)]\n",
    "def concat(text):\n",
    "    concat_text = \" \".join(i for i in text)\n",
    "    return concat_text\n",
    "\n",
    "train_df['Comment'] = train_df['Comment'].astype(str)\n",
    "train_df['Comment'] = train_df['Comment'].apply(lemmatize_text).copy()\n",
    "train_df['Comment'] = train_df['Comment'].apply(lambda x: concat(x))\n",
    "test_df['Comment'] = test_df['Comment'].astype(str)\n",
    "test_df['Comment'] = test_df['Comment'].apply(lemmatize_text).copy()\n",
    "test_df['Comment'] = test_df['Comment'].apply(lambda x: concat(x))\n",
    "\n",
    "#Using simple CountVectorizer.\n",
    "count_vectorizer = CountVectorizer()\n",
    "bowX_train = count_vectorizer.fit_transform(train_df['Comment'].tolist()) \n",
    "bowX_test = count_vectorizer.transform(test_df['Comment'].tolist()) \n",
    "\n",
    "#Forming labels, which are going to help us with prediction and evaluation.\n",
    "label_train = train_df['Insult'].tolist()\n",
    "label_test = test_df['Insult'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gluBkPh7Rs4T"
   },
   "source": [
    "##### Naive Bayes scores with lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14154,
     "status": "ok",
     "timestamp": 1591785565519,
     "user": {
      "displayName": "Σπύρος Μπριάκος",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1ZSGs85hEnrevgOWpVj9cYnDNg1wvvPDbnumHGw=s64",
      "userId": "14497433372697123826"
     },
     "user_tz": -180
    },
    "id": "NfoQFsZmRsZJ",
    "outputId": "e4d463c2-f2c5-4be2-cc9c-0695d0b5146b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of Bag-of-words (lemmatization) with Naive-Bayes Classifier:  0.51\n",
      "Accuracy score of Bag-of-words (lemmatization) with Naive-Bayes Classifier:  0.52\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "\n",
    "#Using Naive Bayes Classifier and train data, then predict and evaluate score on test data.\n",
    "nb = GaussianNB() \n",
    "nb.fit(bowX_train.toarray(), label_train)\n",
    "predictions = nb.predict(bowX_test.toarray())\n",
    "\n",
    "#F1 \n",
    "nb_f1_lemma_score = \"{:.2f}\".format(f1_score(label_test,predictions,average='weighted'))\n",
    "print('F1 score of Bag-of-words (lemmatization) with Naive-Bayes Classifier: ' , nb_f1_lemma_score)\n",
    "\n",
    "#Accuracy\n",
    "nb_acc_lemma_score = \"{:.2f}\".format(accuracy_score(label_test,predictions))\n",
    "print('Accuracy score of Bag-of-words (lemmatization) with Naive-Bayes Classifier: ' , nb_acc_lemma_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Mp4XLw_Rho4"
   },
   "source": [
    "#### Obtain cleaned train,test dataframes, stopwords and then Vectorization with BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "maZDyovHRgJe"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train_df = pd.read_csv(r'/content/drive/My Drive/cleaned_train.csv')\n",
    "test_df = pd.read_csv(r'/content/drive/My Drive/cleaned_test.csv') \n",
    "\n",
    "train_df['Comment'] = train_df['Comment'].astype('U')\n",
    "test_df['Comment'] = test_df['Comment'].astype('U')\n",
    "\n",
    "#Using simple CountVectorizer.\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "bowX_train = count_vectorizer.fit_transform(train_df['Comment'].tolist()) \n",
    "bowX_test = count_vectorizer.transform(test_df['Comment'].tolist()) \n",
    "\n",
    "#Forming labels, which are going to help us with prediction and evaluation.\n",
    "label_train = train_df['Insult'].tolist()\n",
    "label_test = test_df['Insult'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vx_3kBLlTN1i"
   },
   "source": [
    "##### Naive Bayes scores with removal of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16208,
     "status": "ok",
     "timestamp": 1591785567591,
     "user": {
      "displayName": "Σπύρος Μπριάκος",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1ZSGs85hEnrevgOWpVj9cYnDNg1wvvPDbnumHGw=s64",
      "userId": "14497433372697123826"
     },
     "user_tz": -180
    },
    "id": "y99_qpz6L6oK",
    "outputId": "f43c9553-fbaa-415b-c3ba-232aad51e2a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of Bag-of-words (stopwords) with Naive-Bayes Classifier:  0.53\n",
      "Accuracy score of Bag-of-words (stopwords) with Naive-Bayes Classifier:  0.53\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "\n",
    "#Using Naive Bayes Classifier and train data, then predict and evaluate score on test data.\n",
    "nb = GaussianNB() \n",
    "nb.fit(bowX_train.toarray(), label_train)\n",
    "predictions = nb.predict(bowX_test.toarray())\n",
    "\n",
    "#F1 \n",
    "nb_f1_stop_score = \"{:.2f}\".format(f1_score(label_test,predictions,average='weighted'))\n",
    "print('F1 score of Bag-of-words (stopwords) with Naive-Bayes Classifier: ' , nb_f1_stop_score)\n",
    "\n",
    "#Accuracy\n",
    "nb_acc_stop_score = \"{:.2f}\".format(accuracy_score(label_test,predictions))\n",
    "print('Accuracy score of Bag-of-words (stopwords) with Naive-Bayes Classifier: ' , nb_acc_stop_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DlN9ErlRlrsE"
   },
   "source": [
    "#### Obtain cleaned train,test dataframes, bigrams and then Vectorization with BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ObWS1cr7U5mq"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train_df = pd.read_csv(r'/content/drive/My Drive/cleaned_train.csv')\n",
    "test_df = pd.read_csv(r'/content/drive/My Drive/cleaned_test.csv') \n",
    "\n",
    "train_df['Comment'] = train_df['Comment'].astype('U')\n",
    "test_df['Comment'] = test_df['Comment'].astype('U')\n",
    "\n",
    "#Using simple CountVectorizer.\n",
    "count_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "bowX_train = count_vectorizer.fit_transform(train_df['Comment'].tolist()) \n",
    "bowX_test = count_vectorizer.transform(test_df['Comment'].tolist()) \n",
    "\n",
    "#Forming labels, which are going to help us with prediction and evaluation.\n",
    "label_train = train_df['Insult'].tolist()\n",
    "label_test = test_df['Insult'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Zm3bvArU5XP"
   },
   "source": [
    "##### Naive Bayes scores with bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25241,
     "status": "ok",
     "timestamp": 1591785576639,
     "user": {
      "displayName": "Σπύρος Μπριάκος",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1ZSGs85hEnrevgOWpVj9cYnDNg1wvvPDbnumHGw=s64",
      "userId": "14497433372697123826"
     },
     "user_tz": -180
    },
    "id": "mgmCFXRwU44E",
    "outputId": "8e51d26a-ac51-41cf-dd35-7a39ae576a63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of Bag-of-words (bigrams) with Naive-Bayes Classifier:  0.56\n",
      "Accuracy score of Bag-of-words (bigrams) with Naive-Bayes Classifier:  0.56\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "\n",
    "#Using Naive Bayes Classifier and train data, then predict and evaluate score on test data.\n",
    "nb = GaussianNB() \n",
    "nb.fit(bowX_train.toarray(), label_train)\n",
    "predictions = nb.predict(bowX_test.toarray())\n",
    "\n",
    "#F1 \n",
    "nb_f1_bigram_score = \"{:.2f}\".format(f1_score(label_test,predictions,average='weighted'))\n",
    "print('F1 score of Bag-of-words (bigrams) with Naive-Bayes Classifier: ' , nb_f1_bigram_score)\n",
    "\n",
    "#Accuracy\n",
    "nb_acc_bigram_score = \"{:.2f}\".format(accuracy_score(label_test,predictions))\n",
    "print('Accuracy score of Bag-of-words (bigrams) with Naive-Bayes Classifier: ' , nb_acc_bigram_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nU2MiuPGVYTK"
   },
   "source": [
    "#### Obtain cleaned train,test dataframes, Laplace Smoothing and then Vectorization with BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7f31cKSHVZL_"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train_df = pd.read_csv(r'/content/drive/My Drive/cleaned_train.csv')\n",
    "test_df = pd.read_csv(r'/content/drive/My Drive/cleaned_test.csv') \n",
    "\n",
    "train_df['Comment'] = train_df['Comment'].astype('U')\n",
    "test_df['Comment'] = test_df['Comment'].astype('U')\n",
    "\n",
    "#Using simple CountVectorizer.\n",
    "count_vectorizer = CountVectorizer()\n",
    "bowX_train = count_vectorizer.fit_transform(train_df['Comment'].tolist()) \n",
    "bowX_test = count_vectorizer.transform(test_df['Comment'].tolist()) \n",
    "\n",
    "#Forming labels, which are going to help us with prediction and evaluation.\n",
    "label_train = train_df['Insult'].tolist()\n",
    "label_test = test_df['Insult'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "79TezpOMVwt0"
   },
   "source": [
    "##### Naive Bayes scores with Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26517,
     "status": "ok",
     "timestamp": 1591785577932,
     "user": {
      "displayName": "Σπύρος Μπριάκος",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1ZSGs85hEnrevgOWpVj9cYnDNg1wvvPDbnumHGw=s64",
      "userId": "14497433372697123826"
     },
     "user_tz": -180
    },
    "id": "jIBe2sT-Vvky",
    "outputId": "9dd83252-5116-414b-8f0e-8c7158ac1861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of Bag-of-words with Multinomial Naive-Bayes Classifier:  0.67\n",
      "Accuracy score of Bag-of-words) with Multinomial Naive-Bayes Classifier:  0.68\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "\n",
    "#Using Multinomial Naive Bayes Classifier and train data, then predict and evaluate score on test data.\n",
    "mnb = MultinomialNB() #default alpha=1.0, which enables Laplace Smoothing.\n",
    "mnb.fit(bowX_train.toarray(), label_train)\n",
    "predictions = mnb.predict(bowX_test.toarray())\n",
    "\n",
    "#F1 \n",
    "nb_f1_laplace_score = \"{:.2f}\".format(f1_score(label_test,predictions,average='weighted'))\n",
    "print('F1 score of Bag-of-words with Multinomial Naive-Bayes Classifier: ' , nb_f1_laplace_score)\n",
    "\n",
    "#Accuracy\n",
    "nb_acc_laplace_score = \"{:.2f}\".format(accuracy_score(label_test,predictions))\n",
    "print('Accuracy score of Bag-of-words) with Multinomial Naive-Bayes Classifier: ' , nb_acc_laplace_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LTJQG8A3lrsK"
   },
   "source": [
    "### TF-IDF based characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-zkjI6WOlrsL"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train_df = pd.read_csv(r'/content/drive/My Drive/cleaned_train.csv')\n",
    "test_df = pd.read_csv(r'/content/drive/My Drive/cleaned_test.csv') \n",
    "\n",
    "train_df['Comment'] = train_df['Comment'].astype('U')\n",
    "test_df['Comment'] = test_df['Comment'].astype('U')\n",
    "\n",
    "#Convert to vectors with TF-IDF Vectorizer...\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "tfidfX_train = tfidf_vectorizer.fit_transform(train_df['Comment'].tolist())  \n",
    "tfidfX_test = tfidf_vectorizer.transform(test_df['Comment'].tolist()) \n",
    "\n",
    "#Store to arrays the results of TF-TDF Vectorizer and later we'll add 4 frequencies (adverbs,verbs,adjectives,nouns)\n",
    "train_characteristics_array = tfidfX_train.toarray()\n",
    "test_characteristics_array = tfidfX_test.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lrl0wn8slrsP"
   },
   "source": [
    "### Part-Of-Speech based characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 37415,
     "status": "ok",
     "timestamp": 1591785588847,
     "user": {
      "displayName": "Σπύρος Μπριάκος",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1ZSGs85hEnrevgOWpVj9cYnDNg1wvvPDbnumHGw=s64",
      "userId": "14497433372697123826"
     },
     "user_tz": -180
    },
    "id": "f8zYgYZYlrsQ",
    "outputId": "0b7558d4-b82a-46b9-9274-04c07e3078d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#Create two empty lists which we'll fill with info in order to have 2 complex arrays (one for train data and one for test data.)\n",
    "final_train_array = []\n",
    "final_test_array = []\n",
    "\n",
    "#Loop through each comment...\n",
    "for i, row in train_df.iterrows():\n",
    "    adverbs = 0.00\n",
    "    verbs = 0.00\n",
    "    adjectives = 0.00\n",
    "    nouns = 0.00\n",
    "    pos_list=[]\n",
    "    #Tokenize each word of each comment ant then with pos_tag from nltk keep in a list info of kind of each word.\n",
    "    pos_list = nltk.word_tokenize(train_df.loc[i,'Comment'])\n",
    "    pos_list = nltk.pos_tag(pos_list)\n",
    "    \n",
    "    #Keep info for number of adverbs,verbs,adjectives,nouns in a comment.\n",
    "    for tuple_ in pos_list:\n",
    "        if tuple_[1]==\"RB\" or tuple_[1]==\"RBR\" or tuple_[1]==\"RBS\":\n",
    "            adverbs = adverbs + 1\n",
    "        if tuple_[1]==\"VBP\" or tuple_[1]==\"VB\" or tuple_[1]==\"VBN\" or tuple_[1]==\"VBG\" or tuple_[1]==\"VBD\" or tuple_[1]==\"VBZ\":\n",
    "            verbs = verbs + 1\n",
    "        if tuple_[1]==\"JJ\" or tuple_[1]==\"JJR\" or tuple_[1]==\"JJS\":\n",
    "            adjectives = adjectives + 1\n",
    "        if tuple_[1]==\"NN\" or tuple_[1]==\"NNS\":\n",
    "            nouns = nouns + 1            \n",
    "    #If we have info calculate percentages of adverbs,verbs,adjectives,nouns based on the sum of words into a comment.\n",
    "    if len(pos_list)!=0:\n",
    "        adverbs = adverbs/len(pos_list)\n",
    "        verbs = verbs/len(pos_list)   \n",
    "        adjectives = adjectives/len(pos_list)  \n",
    "        nouns = nouns/len(pos_list) \n",
    "\n",
    "    #Store in a temporary list characteristics of TF-IDF and 4 percentages at the end of each list (about adverbs,verbs,adjectives,nouns) and then \n",
    "    #append this temp_list into the final 'complex' list.\n",
    "    temp_list = []\n",
    "    temp_list = train_characteristics_array[i].tolist()\n",
    "    temp_list.append(adverbs)\n",
    "    temp_list.append(verbs)\n",
    "    temp_list.append(adjectives)\n",
    "    temp_list.append(nouns)\n",
    "    final_train_array.append(temp_list)\n",
    "\n",
    "#Here we are doing the same process as before, but now for test data...\n",
    "for i, row in test_df.iterrows():\n",
    "    adverbs = 0.00\n",
    "    verbs = 0.00\n",
    "    adjectives = 0.00\n",
    "    nouns = 0.00\n",
    "    pos_list=[]\n",
    "    pos_list = nltk.word_tokenize(test_df.loc[i,'Comment'])\n",
    "    pos_list = nltk.pos_tag(pos_list)\n",
    "    \n",
    "    for tuple_ in pos_list:\n",
    "        if tuple_[1]==\"RB\" or tuple_[1]==\"RBR\" or tuple_[1]==\"RBS\":\n",
    "            adverbs = adverbs + 1\n",
    "        if tuple_[1]==\"VBP\" or tuple_[1]==\"VB\" or tuple_[1]==\"VBN\" or tuple_[1]==\"VBG\" or tuple_[1]==\"VBD\" or tuple_[1]==\"VBZ\":\n",
    "            verbs = verbs + 1\n",
    "        if tuple_[1]==\"JJ\" or tuple_[1]==\"JJR\" or tuple_[1]==\"JJS\":\n",
    "            adjectives = adjectives + 1\n",
    "        if tuple_[1]==\"NN\" or tuple_[1]==\"NNS\":\n",
    "            nouns = nouns + 1            \n",
    "    \n",
    "    if len(pos_list)!=0:\n",
    "        adverbs = adverbs/len(pos_list)\n",
    "        verbs = verbs/len(pos_list)    \n",
    "        adjectives = adjectives/len(pos_list)\n",
    "        nouns = nouns/len(pos_list)\n",
    "    \n",
    "    temp_list = []\n",
    "    temp_list = test_characteristics_array[i].tolist()\n",
    "    temp_list.append(adverbs)\n",
    "    temp_list.append(verbs)\n",
    "    temp_list.append(adjectives)\n",
    "    temp_list.append(nouns)\n",
    "    final_test_array.append(temp_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dgWFkzlMxNV5"
   },
   "source": [
    "### TFIDF+POS complex array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 37407,
     "status": "ok",
     "timestamp": 1591785588848,
     "user": {
      "displayName": "Σπύρος Μπριάκος",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1ZSGs85hEnrevgOWpVj9cYnDNg1wvvPDbnumHGw=s64",
      "userId": "14497433372697123826"
     },
     "user_tz": -180
    },
    "id": "j9uqVIuwlrsV",
    "outputId": "86a8da33-4fd1-4e45-e72f-1da9d0ac6e81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF train's shape:  (3947, 1000)\n",
      "Final train complex array's shape:  (3947, 1004)\n",
      "TF-IDF test's shape: (2235, 1000)\n",
      "Final complex test array's shape:  (2235, 1004)\n"
     ]
    }
   ],
   "source": [
    "#Convert lists of lists to np.arrays, cause we are going to use them later (on SVM & Random Forest classifier)\n",
    "train_complex_array = np.array(final_train_array)\n",
    "test_complex_array = np.array(final_test_array)\n",
    "\n",
    "#Print some useful infos so as to prove that we have added infos about percentages of adverbs,verbs,adjectives,nouns.\n",
    "print(\"TF-IDF train's shape: \", train_characteristics_array.shape)\n",
    "print(\"Final train complex array's shape: \", train_complex_array.shape)\n",
    "print(\"TF-IDF test's shape:\", test_characteristics_array.shape)\n",
    "print(\"Final complex test array's shape: \", test_complex_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BELtjVtzypSy"
   },
   "source": [
    "### Now we are going to try SVM and Random Forest classifiers with complex arrays that we have just created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ra2IwQASysdM"
   },
   "source": [
    "#### 1) SVM Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 63306,
     "status": "ok",
     "timestamp": 1591785614755,
     "user": {
      "displayName": "Σπύρος Μπριάκος",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1ZSGs85hEnrevgOWpVj9cYnDNg1wvvPDbnumHGw=s64",
      "userId": "14497433372697123826"
     },
     "user_tz": -180
    },
    "id": "CnekYwhNywzP",
    "outputId": "b42d3615-d67f-4916-f89f-3d5a8abe79e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of TF-IDF and POS based features with SVM Classifier:  0.64\n",
      "Accuracy score of TF-IDF and POS based features with SVM Classifier:  0.67\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm \n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "\n",
    "#Using SVM Classifier and train data, then predict and evaluate score on test data.\n",
    "svm_clf = svm.SVC()\n",
    "svm_clf.fit(train_complex_array, label_train)\n",
    "predictions = svm_clf.predict(test_complex_array)\n",
    "\n",
    "#F1 \n",
    "first_svm_f1_score = \"{:.2f}\".format(f1_score(label_test,predictions,average='weighted'))\n",
    "print('F1 score of TF-IDF and POS based features with SVM Classifier: ' , first_svm_f1_score)\n",
    "\n",
    "#Accuracy\n",
    "first_svm_acc_score = \"{:.2f}\".format(accuracy_score(label_test,predictions))\n",
    "print('Accuracy score of TF-IDF and POS based features with SVM Classifier: ' , first_svm_acc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rqcal-_Syyoi"
   },
   "source": [
    "#### 2) Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 66267,
     "status": "ok",
     "timestamp": 1591785617724,
     "user": {
      "displayName": "Σπύρος Μπριάκος",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1ZSGs85hEnrevgOWpVj9cYnDNg1wvvPDbnumHGw=s64",
      "userId": "14497433372697123826"
     },
     "user_tz": -180
    },
    "id": "iAM0kkGby1iU",
    "outputId": "b66d374d-6ca8-4250-ce61-5c84cacd5e27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of TF-IDF and POS based features with Random Forest Classifier:  0.60\n",
      "Accuracy score of TF-IDF and POS based features with Random Forest Classifier:  0.64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "\n",
    "#Using Random Forest Classifier and train data, then predict and evaluate score on test data.\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(train_complex_array, label_train)\n",
    "predictions = rf.predict(test_complex_array)\n",
    "\n",
    "#F1 \n",
    "first_rf_f1_score = \"{:.2f}\".format(f1_score(label_test,predictions,average='weighted'))\n",
    "print('F1 score of TF-IDF and POS based features with Random Forest Classifier: ' , first_rf_f1_score)\n",
    "\n",
    "#Accuracy\n",
    "first_rf_acc_score = \"{:.2f}\".format(accuracy_score(label_test,predictions))\n",
    "print('Accuracy score of TF-IDF and POS based features with Random Forest Classifier: ' , first_rf_acc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pWwZ7uHMG-mg"
   },
   "source": [
    "\n",
    "### Beat the Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mqq_LLZoleqZ"
   },
   "source": [
    "#### Lemmatization again on clean_data (train & test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 66638,
     "status": "ok",
     "timestamp": 1591785618106,
     "user": {
      "displayName": "Σπύρος Μπριάκος",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1ZSGs85hEnrevgOWpVj9cYnDNg1wvvPDbnumHGw=s64",
      "userId": "14497433372697123826"
     },
     "user_tz": -180
    },
    "id": "R62dz-JLj6yR",
    "outputId": "7375336b-6764-4d3b-a5ab-71fe0264ea8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "train_df = pd.read_csv(r'/content/drive/My Drive/cleaned_train.csv')\n",
    "test_df = pd.read_csv(r'/content/drive/My Drive/cleaned_test.csv') \n",
    "\n",
    "# Lemmatization on data\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "#Functions, first lemmatize a word and second just concatenate strings of a list to a simple string.\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w, pos=\"v\") for w in w_tokenizer.tokenize(text)]\n",
    "def concat(text):\n",
    "    concat_text = \" \".join(i for i in text)\n",
    "    return concat_text\n",
    "\n",
    "train_df['Comment'] = train_df['Comment'].astype(str)\n",
    "train_df['Comment'] = train_df['Comment'].apply(lemmatize_text).copy()\n",
    "train_df['Comment'] = train_df['Comment'].apply(lambda x: concat(x))\n",
    "test_df['Comment'] = test_df['Comment'].astype(str)\n",
    "test_df['Comment'] = test_df['Comment'].apply(lemmatize_text).copy()\n",
    "test_df['Comment'] = test_df['Comment'].apply(lambda x: concat(x))\n",
    "\n",
    "#Forming labels, which are going to help us with prediction and evaluation.\n",
    "label_train = train_df['Insult'].tolist()\n",
    "label_test = test_df['Insult'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1hWBPhBGlsoi"
   },
   "source": [
    "#### TFIDF Vectorizer and added info about pronouns of each comment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 110192,
     "status": "ok",
     "timestamp": 1591785661668,
     "user": {
      "displayName": "Σπύρος Μπριάκος",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1ZSGs85hEnrevgOWpVj9cYnDNg1wvvPDbnumHGw=s64",
      "userId": "14497433372697123826"
     },
     "user_tz": -180
    },
    "id": "5BiORW8HHBJn",
    "outputId": "27098ca3-3b36-4882-f65e-ab4f7b2c763f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of TF-IDF and pronoun-added feature with SVM Classifier:  0.64\n",
      "Accuracy score of TF-IDF and pronoun-added feature with SVM Classifier:  0.71\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm \n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "\n",
    "#Convert to vectors with TF-IDF Vectorizer...\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1700)\n",
    "tfidfX_train = tfidf_vectorizer.fit_transform(train_df['Comment'].tolist())  \n",
    "tfidfX_test = tfidf_vectorizer.transform(test_df['Comment'].tolist()) \n",
    "\n",
    "#Convert results of TFIDF Vectorizer to arrays.\n",
    "train_characteristics_array = tfidfX_train.toarray()\n",
    "test_characteristics_array = tfidfX_test.toarray()\n",
    "\n",
    "#Create two empty lists which we'll fill with info in order to have 2 complex arrays (one for train data and one for test data.)\n",
    "final_train_array = []\n",
    "final_test_array = []\n",
    "\n",
    "#Loop through each comment...\n",
    "for i, row in train_df.iterrows():\n",
    "    pronouns=0\n",
    "    pos_list=[]\n",
    "    \n",
    "    pos_list = nltk.word_tokenize(train_df.loc[i,'Comment'])\n",
    "    pos_list1 = nltk.pos_tag(pos_list)\n",
    "    for tuple_ in pos_list1:\n",
    "      if tuple_[1]==\"PRP\" or tuple_[1]==\"PRP$\":\n",
    "          pronouns = pronouns + 1  \n",
    "    \n",
    "    #Store in a temporary list characteristics of TF-IDF and number of pronouns of each list.\n",
    "    temp_list = []\n",
    "    temp_list = train_characteristics_array[i].tolist()\n",
    "    temp_list.append(pronouns)\n",
    "    final_train_array.append(temp_list)\n",
    "\n",
    "#Here we are doing the same process as before, but now for test data...\n",
    "for i, row in test_df.iterrows():\n",
    "    pronouns = 0\n",
    "    pos_list=[]\n",
    "    \n",
    "    pos_list = nltk.word_tokenize(test_df.loc[i,'Comment'])\n",
    "    pos_list1 = nltk.pos_tag(pos_list)\n",
    "    #For each comment find how many pronouns exist and store it to pronouns, so as to add it as an extra feature.\n",
    "    for tuple_ in pos_list1:\n",
    "      if tuple_[1]==\"PRP\" or tuple_[1]==\"PRP$\":\n",
    "          pronouns = pronouns + 1\n",
    "    \n",
    "    temp_list = []\n",
    "    temp_list = test_characteristics_array[i].tolist()\n",
    "    temp_list.append(pronouns)\n",
    "    final_test_array.append(temp_list)\n",
    "\n",
    "\n",
    "train_addedinfo_array = np.array(final_train_array)\n",
    "test_addedinfo_array = np.array(final_test_array)\n",
    "\n",
    "#Using SVM Classifier and train data, then predict and evaluate score on test data.\n",
    "svm_clf = svm.SVC(kernel='rbf',C=1690,gamma=0.001)\n",
    "svm_clf.fit(train_addedinfo_array, label_train)\n",
    "predictions = svm_clf.predict(test_addedinfo_array)\n",
    "\n",
    "second_svm_f1_score = \"{:.2f}\".format(f1_score(label_test,predictions),average='weighted')\n",
    "print('F1 score of TF-IDF and pronoun-added feature with SVM Classifier: ' , second_svm_f1_score)\n",
    "second_svm_acc_score = \"{:.2f}\".format(accuracy_score(label_test,predictions))\n",
    "print('Accuracy score of TF-IDF and pronoun-added feature with SVM Classifier: ' , second_svm_acc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mG-LTK34HDCW"
   },
   "source": [
    "### Results & Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dhHqHcsrs778"
   },
   "source": [
    "#### Results & Conclusions about Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1301,
     "status": "ok",
     "timestamp": 1591785684192,
     "user": {
      "displayName": "Σπύρος Μπριάκος",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1ZSGs85hEnrevgOWpVj9cYnDNg1wvvPDbnumHGw=s64",
      "userId": "14497433372697123826"
     },
     "user_tz": -180
    },
    "id": "yb7cAhHGCTYH",
    "outputId": "ecae64aa-fa3a-4f29-ac6a-3984f0491fe4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scorer                 F1    Accuracy\n",
      "BOW                    0.53  0.53\n",
      "BOW+Lemmatization:     0.51  0.52\n",
      "BOW+StopWords:         0.53  0.53\n",
      "BOW+Bigrams:           0.56  0.56\n",
      "BOW+Laplace Smoothing: 0.67  0.68\n"
     ]
    }
   ],
   "source": [
    "print(\"Scorer                 F1    Accuracy\")\n",
    "print(\"BOW                   \",first_nb_f1_score,\"\",first_nb_acc_score)\n",
    "print(\"BOW+Lemmatization:    \",nb_f1_lemma_score,\"\",nb_acc_lemma_score)\n",
    "print(\"BOW+StopWords:        \",nb_f1_stop_score,\"\",nb_acc_stop_score)\n",
    "print(\"BOW+Bigrams:          \",nb_f1_bigram_score,\"\",nb_acc_bigram_score)\n",
    "print(\"BOW+Laplace Smoothing:\",nb_f1_laplace_score,\"\",nb_acc_laplace_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L0vtOEXevkX1"
   },
   "source": [
    "#####  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "* Firstly we have to refer that our dataset (train & test data) is made up of Comments, which through thorough observation of them I concluded that a quite large portion of Comments \n",
    "uses oral language (like slang or cant). \n",
    "* So this unique whim of our dataset makes our job extremely difficult, cause there are a lot of words which are completely uknown to our models and some of them they are used rarely \n",
    "(such as: \"How old are u 12? \\n\\nKnicks fan **BEEYATCH**!!!\\n\\nNot a Melo ball licker!!!\").\n",
    "). \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "1) Our NB model has the **worst** score when we applied, to our dataset, lemmatization and the explanation of this performance is that lemmatization \n",
    "maybe is affected in a small scale by this unique whim of our dataset.\n",
    "\n",
    "\n",
    "---\n",
    "2) Our NB model has the **best** score when we applied, to our dataset, MultinomialNB (alpha=1.0), which practically means that we applied Laplace smoothing. The reason of this good score compared with previous techniques is that has the value alpha=1.0 and this practically means that probability of each word cannot be zero, so with that way we regularize Naive Bayes. This helps us cause words that are used rarely would have probability zero in simple Naive Bayes and this would have a bad impact in the prediction of each comment, but with Laplace Smoothing we are 'giving the chance' to words that are used very commonly to play the most significant role and thus these predict more correct Comments.\n",
    "\n",
    "\n",
    "---\n",
    "3) Furthermore, stopwords seems to have the same behavior if with the case of not removing them and this stands by, because maybe model is balancing between of common useful words such as you,your which are thrown away and common completely unuseful words such as and,the.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bXUdyb3RtB4_"
   },
   "source": [
    "#### Results & Conclusions about SVM,Random Forest on TFIDF+PartOfSpeech array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 110174,
     "status": "ok",
     "timestamp": 1591785661670,
     "user": {
      "displayName": "Σπύρος Μπριάκος",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1ZSGs85hEnrevgOWpVj9cYnDNg1wvvPDbnumHGw=s64",
      "userId": "14497433372697123826"
     },
     "user_tz": -180
    },
    "id": "dm-NDbpNrUTz",
    "outputId": "49eea7f7-f095-43a5-8e81-dcdc9ad65340"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scorer                           F1    Accuracy\n",
      "TFIDF+PartOfSpeech+RandomForest: 0.60  0.64\n",
      "TFIDF+PartOfSpeech+SVM:          0.64  0.67\n"
     ]
    }
   ],
   "source": [
    "print(\"Scorer                           F1    Accuracy\")\n",
    "print(\"TFIDF+PartOfSpeech+RandomForest:\",first_rf_f1_score,\"\",first_rf_acc_score)\n",
    "print(\"TFIDF+PartOfSpeech+SVM:         \",first_svm_f1_score,\"\",first_svm_acc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FFWAKOIgvuHe"
   },
   "source": [
    "##### As we can observe... \n",
    "* SVM classifier was better than Random Forest classifier and so does generally accuracy score compared with f1 score.\n",
    "\n",
    "* The reason of this optimization is not only from TFIDF Vectorizer, which worked better than BoW and POS tags which give more info about each comment, but also from SVM and Random Forest Classifier which were more efficient on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PDkm4NtNtNtF"
   },
   "source": [
    "#### Results & Conclusions about Beat the Benchhmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 110134,
     "status": "ok",
     "timestamp": 1591785661671,
     "user": {
      "displayName": "Σπύρος Μπριάκος",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh1ZSGs85hEnrevgOWpVj9cYnDNg1wvvPDbnumHGw=s64",
      "userId": "14497433372697123826"
     },
     "user_tz": -180
    },
    "id": "JW_nrI3_skOd",
    "outputId": "eafefcb7-b8d1-4eee-8916-730e8d5f041c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scorer                        F1    Accuracy\n",
      "TFIDF+Lemmatization+Pronouns: 0.64  0.71\n"
     ]
    }
   ],
   "source": [
    "print(\"Scorer                        F1    Accuracy\")\n",
    "print(\"TFIDF+Lemmatization+Pronouns:\",second_svm_f1_score,\"\",second_svm_acc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__PD5lWbvwXJ"
   },
   "source": [
    "---\n",
    "#####SVM classifier with accuracy score has the **best** score in this notebook and made a quite small optimization (0.67->0.71). \n",
    "\n",
    "* To achive this score i applied lemmatization on our dataset and TFIDF Vectorization (which obviously gave us better scores than BoW).\n",
    "\n",
    "*  Additionally, I calculate number of pronouns for each Comment and added this significant quantity in the end of TFIDF vectorized array. My initiative was made cause I searched on Internet and I found that this dataset is from a specific contest of KAGGLE and in important notes it was emphasized: \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "1) We are looking for comments that are intended to be insulting to a person who is a part of the larger blog/forum conversation. \n",
    "\n",
    "2) We are NOT looking for insults directed to non-participants (such as celebrities, public figures etc.). \n",
    "\n",
    "3) Insults could contain profanity, racial slurs, or other offensive language. But often times, they do not. \n",
    "\n",
    "4) Comments which contain profanity or racial slurs, but are not necessarily insulting to another person are considered not insulting.\" \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "* So for this reason I concluded that,because many labeled as insult Comments have a lot of pronouns, it will play significant role to our final prediction.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "For example this Comment is assumed to be non-insult:\n",
    "\n",
    "\"fuck the judges..better stop the boxing events!\"\t\t\t\t\n",
    "\n",
    "For example this Comment is assumed to be insult:\n",
    "\n",
    "\"You're a fucking joke.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "* Except of all that I referred right before, one extra factor for my improvement in new score is undoubtly parameters of SVM classifier, which I find through after many attempts with random parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "04KEN1HF0zCV"
   },
   "source": [
    "##### Note: In all of our attempts we have done a 'small' cleaning. To be more specific we removed html tags,urls,punctuations and symbols."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "hqS3lDtmM3Bv",
    "CsFtb_Lmlrrh",
    "vzYyBIzAlrrm",
    "ad-5irRFlrrt",
    "GqwK1Mtklrru",
    "N2ho58dFlrrz",
    "1kCG11vylrr6",
    "bexpL-tIlrr8",
    "gluBkPh7Rs4T",
    "4Mp4XLw_Rho4",
    "DlN9ErlRlrsE",
    "0Zm3bvArU5XP",
    "nU2MiuPGVYTK",
    "LTJQG8A3lrsK",
    "lrl0wn8slrsP",
    "dgWFkzlMxNV5",
    "BELtjVtzypSy",
    "pWwZ7uHMG-mg",
    "Mqq_LLZoleqZ",
    "1hWBPhBGlsoi",
    "mG-LTK34HDCW",
    "dhHqHcsrs778",
    "bXUdyb3RtB4_",
    "PDkm4NtNtNtF"
   ],
   "name": "Απαλλακτική_Εργασία.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
